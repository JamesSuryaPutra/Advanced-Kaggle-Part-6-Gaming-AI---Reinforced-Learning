<Introduction>
In the tutorial, you learned a bit about reinforcement learning and used the stable-baselines3 package to train an agent to beat a random opponent. In this exercise, you will check your und
erstanding and tinker with the code to deepen your intuition.

****************
from learntools.core import binder
binder.bind(globals())
from learntools.game_ai.ex4 import *
****************

<1st step: Set the architecture>
In the tutorial, you learned one way to design a neural network that can select moves in Connect Four. The neural network had an output layer with seven nodes: one for each column in the
game board.

Say now you wanted to create a neural network that can play chess. How many nodes should you put in the output layer?
1} Option A: 2 nodes (number of game players)
2} Option B: 16 nodes (number of game pieces that each player starts with)
3} Option C: 4672 nodes (number of possible moves)
4} Option D: 64 nodes (number of squares on the game board)

Use your answer to set the value of the best_option variable below. Your answer should be one of 'A', 'B', 'C', or 'D'.

****************
# Fill in the blank
best_option = 'C'

# Check your answer
q_1.check()

Correct:
If we use a similar network as in the tutorial, the network should output a probability for each possible move.
****************

<2nd step: Decide reward>
In the tutorial, you learned how to give your agent a reward that encourages it to win games of Connect Four. Consider now training an agent to win at the game Minesweeper. The goal of
the game is to clear the board without detonating any bombs.

With each move, one of the following is true:
1} The agent selected an invalid move (in other words, it tried to uncover a square that was uncovered as part of a previous move). Let's assume this ends the game, and the agent loses.
2} The agent clears a square that did not contain a hidden mine. The agent wins the game, because all squares without mines are revealed.
3} The agent clears a square that did not contain a hidden mine, but has not yet won or lost the game.
4} The agent detonates a mine and loses the game.

How might you specify the reward for each of these four cases, so that by maximizing the cumulative reward, the agent will try to win the game? After you have decided on your answer, run
the code cell below to get credit for completing this question.

****************
# Check your answer (Run this code cell to receive credit!)
q_2.solution()

Solution:
Here's a possible solution - after each move, we give the agent a reward that tells it how well it did:
1} If agent wins the game in that move, it gets a reward of +1.
2} Else if the agent selects an invalid move, it gets a reward of -10.
3} Else if it detonates a mine, it gets a reward of -1.
4} Else if the agent clears a square with no hidden mine, it gets a reward of +1/100.

To check the validity of your answer, note that the reward for selecting an invalid move and for detonating a mine should both be negative. The reward for winning the game should be posi
tive. And, the reward for clearing a square with no hidden mine should be either zero or slightly positive.
****************

<3rd step - Optional: Amend the code>
In this next part of the exercise, you will amend the code from the tutorial to experiment with creating your own agents! There are a lot of hyperparameters involved with specifying a
reinforcement learning agent, and you'll have a chance to amend them, to see how performance is affected.

Begin by running the code cell below.

****************
import random
import numpy as np
import pandas as pd
import gym
import matplotlib.pyplot as plt
%matplotlib inline

from kaggle_environments import make, evaluate
from gym import spaces

class ConnectFourGym(gym.Env):
    def __init__(self, agent2="random"):
        ks_env = make("connectx", debug=True)
        self.env = ks_env.train([None, agent2])
        self.rows = ks_env.configuration.rows
        self.columns = ks_env.configuration.columns
        # Learn about spaces here: http://gym.openai.com/docs/#spaces
        self.action_space = spaces.Discrete(self.columns)
        self.observation_space = spaces.Box(low=0, high=2, 
                                            shape=(1,self.rows,self.columns), dtype=int)
        # Tuple corresponding to the min and max possible rewards
        self.reward_range = (-10, 1)
        # StableBaselines throws error if these are not defined
        self.spec = None
        self.metadata = None
    def reset(self):
        self.obs = self.env.reset()
        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)
    def change_reward(self, old_reward, done):
        if old_reward == 1: # The agent won the game
            return 1
        elif done: # The opponent won the game
            return -1
        else: # Reward 1/42
            return 1/(self.rows*self.columns)
    def step(self, action):
        # Check if agent's move is valid
        is_valid = (self.obs['board'][int(action)] == 0)
        if is_valid: # Play the move
            self.obs, old_reward, done, _ = self.env.step(int(action))
            reward = self.change_reward(old_reward, done)
        else: # End the game and penalize agent
            reward, done, _ = -10, True, {}
        return np.array(self.obs['board']).reshape(1,self.rows,self.columns), reward, done, _

# Create ConnectFour environment 
env = ConnectFourGym(agent2="random")

import torch as th
import torch.nn as nn

!pip install "stable-baselines3"
from stable_baselines3 import PPO 
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor

# Neural network for predicting action values
class CustomCNN(BaseFeaturesExtractor):
    
    def __init__(self, observation_space: gym.spaces.Box, features_dim: int=128):
        super(CustomCNN, self).__init__(observation_space, features_dim)
        # CxHxW images (channels first)
        n_input_channels = observation_space.shape[0]
        self.cnn = nn.Sequential(
            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=0),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),
            nn.ReLU(),
            nn.Flatten(),
        )

        # Compute shape by doing one forward pass
        with th.no_grad():
            n_flatten = self.cnn(
                th.as_tensor(observation_space.sample()[None]).float()
            ).shape[1]

        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())

    def forward(self, observations: th.Tensor) -> th.Tensor:
        return self.linear(self.cnn(observations))

policy_kwargs = dict(
    features_extractor_class=CustomCNN,
)

No pygame installed, ignoring import

Requirement already satisfied: stable-baselines3 in /opt/conda/lib/python3.10/site-packages (2.1.0)
Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (0.29.0)
Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (1.26.4)
Requirement already satisfied: torch>=1.13 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (2.1.2+cpu)
Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (2.2.1)
Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (2.2.1)
Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (3.7.5)
Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.9.0)
Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)
Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.13.1)
Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (1.12)
Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.2.1)
Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.1.2)
Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (2024.3.0)
Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (1.2.0)
Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (0.12.1)
Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (4.47.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (1.4.5)
Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (21.3)
Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (9.5.0)
Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (3.1.1)
Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->stable-baselines3) (2023.3.post1)
Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->stable-baselines3) (2023.4)
Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)
Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.3)
Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)
****************

Next, run the code cell below to train an agent with PPO. This code is identical to the code from the tutorial.

****************
# Initialize agent
model = PPO("CnnPolicy", env, policy_kwargs=policy_kwargs, verbose=0)

# Train agent
model.learn(total_timesteps=50000)

<stable_baselines3.ppo.ppo.PPO at 0x7a42762a31c0>
****************

Once you have verified that the code runs, try making amendments to see if you can get increased performance. You might like to:
1} Learn more about the stable-baselines3 package to amend the agent.
2} Change agent2 to a different agent when creating the ConnectFour environment with env = ConnectFourGym(agent2="random"). For instance, you might like to use the "negamax" agent, or
a different, custom agent. Note that the smarter you make the opponent, the harder it will be for your agent to train!
